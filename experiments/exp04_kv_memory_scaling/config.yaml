# Experiment 4: KV Cache vs BBPM Memory Scaling

# Transformer settings (for KV cache calculation)
num_layers: 32
num_heads: 32
head_dim: 128
batch_size: 1
dtype_bytes: 2  # float16

# BBPM settings
D: 10000000  # 10M slots
d: 32
K: 32
H: 1

# Context lengths to test
context_lengths: [1000, 3000, 5000, 10000, 20000, 30000, 40000]

# Reproducibility
seed: 42

# Device
device: "auto"
