# Experiment 7: LLM Integration with Controller-Based Retrieval Injection

# BBPM settings - stress sweep to show degradation
D_list: [50000, 100000, 200000]  # Memory sizes to test
d: 64  # Will be overridden by model.config.hidden_size if transformers available
K_list: [4, 16, 64]  # Sparsity factors (stress test: K=16, H=1 for main sweep; K=64 to show collapse)
H_list: [1]  # Number of hashes (optionally [1, 3] for stress)

# LLM settings
model_name: "gpt2"  # Small model for CPU compatibility
window_size: 50  # Sliding window size for baseline prompt

# Test settings
N_list: [1000, 2000, 5000, 10000, 20000]  # Number of facts to stream
num_queries: 30  # Number of queries to test (increased for better stats)
max_new_tokens: 16  # Max tokens to generate (reduced for consistency)

# Reproducibility (multiple seeds for error bars)
seeds: [0, 1, 2]

# Device
device: "auto"
