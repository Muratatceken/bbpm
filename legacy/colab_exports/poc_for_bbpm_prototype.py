# -*- coding: utf-8 -*-
"""PoC for BBPM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14d5c6jPhamPab_3YAHEd858yjMiNYCez

# Differentiable Hash Tables

##The "Gradient Flow" Test
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class DifferentiableHashMap(nn.Module):
    def __init__(self, memory_size=1000, embedding_dim=1):
        super().__init__()
        # HafÄ±za parametreleri (EÄŸitilebilir Payload)
        self.memory = nn.Parameter(torch.randn(memory_size, embedding_dim))
        self.size = memory_size

    def hash_idx(self, x):
        # Deterministik Hash (TÃ¼revsiz)
        return (x * 123456789).long() % self.size

    def forward(self, x):
        indices = self.hash_idx(x)
        return self.memory[indices]

# Deney Kurulumu
# Girdileri normalize ediyoruz (0.0 ile 1.0 arasÄ±)
# Bu, Gradient Explosion'Ä± Ã¶nler.
# Execution code moved to main()

"""##The "Speed vs. Scale" Benchmark"""

import time
import torch.nn.functional as F

# Execution code moved to main()

# Dummy DHM Operation (Index Lookup)
def dhm_op(x, mem_size=1000000):
    idx = (x * 12345).long() % mem_size
    return idx

# Dummy Attention Operation (Matmul)
def attn_op(x):
    # Q * K^T (N x N matris Ã§arpÄ±mÄ±)
    return torch.matmul(x, x.transpose(-2, -1))

# Execution code moved to main()
# for N in seq_lengths:
    x = torch.randn(1, N, 64) # Batch=1, Seq=N, Dim=64

    # 1. Attention Benchmark
    start = time.time()
    try:
        if N < 15000: # 15k Ã¼stÃ¼ CPU/GPU'yu kilitleyebilir
            _ = attn_op(x)
        else:
            raise MemoryError("OOM") # SimÃ¼le edilmiÅŸ OOM
        attn_times.append(time.time() - start)
    except:
        attn_times.append(None) # PatladÄ±

    # 2. DHM Benchmark
    start = time.time()
    _ = dhm_op(x[:, :, 0]) # Sadece indeksleme
    dhm_times.append(time.time() - start)

# Execution code moved to main()

"""##The "Collision Tolerance" Test"""

# Execution code moved to main()

"""##The "Universal Neural Memory"
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np

# --- 1. MODALITY ADAPTER (GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleyici) ---
class ImageTokenizer(nn.Module):
    """
    Bu sÄ±nÄ±f, bir VQ-VAE (Vector Quantized VAE) simÃ¼lasyonudur.
    GÃ¶revi: SÃ¼rekli sinyali (GÃ¶rÃ¼ntÃ¼) ayrÄ±k parÃ§alara (Token) Ã§evirmek.
    """
    def __init__(self, image_size=64, patch_size=8, embed_dim=32):
        super().__init__()
        self.H = image_size
        self.W = image_size
        self.P = patch_size
        self.embed_dim = embed_dim

        # Bir gÃ¶rÃ¼ntÃ¼de kaÃ§ tane patch (token) olacak?
        # 64x64 resim, 8x8'lik patchlere bÃ¶lÃ¼nÃ¼rse -> 8x8 = 64 Token eder.
        self.num_patches = (self.H // self.P) * (self.W // self.P)

        # Basit bir Encoder/Decoder SimÃ¼lasyonu (Linear Projection)
        # GerÃ§ek hayatta burasÄ± CNN veya ViT olurdu.
        self.patch_encoder = nn.Linear(patch_size * patch_size, embed_dim)
        self.patch_decoder = nn.Linear(embed_dim, patch_size * patch_size)

    def image_to_tokens(self, image):
        # Image: [Batch, H, W] -> Patches: [Batch, Num_Patches, Patch_Size^2]
        # Resmi Ä±zgara (grid) gibi parÃ§alara bÃ¶lÃ¼yoruz
        b, h, w = image.shape
        patches = image.unfold(1, self.P, self.P).unfold(2, self.P, self.P)
        patches = patches.contiguous().view(b, -1, self.P * self.P)

        # Her patch'i bir "Embedding"e Ã§evir (Tokenize et)
        tokens = self.patch_encoder(patches)
        return tokens # [Batch, 64, 32]

    def tokens_to_image(self, tokens):
        # Embedding -> Pixels
        reconstructed_patches = self.patch_decoder(tokens)

        # Patchleri tekrar resim formatÄ±nda birleÅŸtir (Fold)
        b = tokens.shape[0]
        # Pytorch'ta 'fold' iÅŸlemi biraz karÄ±ÅŸÄ±ktÄ±r, burada manuel reshape yapÄ±yoruz
        grid_size = self.H // self.P

        # [Batch, Grid_H, Grid_W, Patch_H, Patch_W]
        rec_img = reconstructed_patches.view(b, grid_size, grid_size, self.P, self.P)

        # [Batch, Grid_H, Patch_H, Grid_W, Patch_W] -> [Batch, H, W]
        rec_img = rec_img.permute(0, 1, 3, 2, 4).contiguous()
        rec_img = rec_img.view(b, self.H, self.W)
        return rec_img

# --- 2. THE PRIMITIVE: DIFFERENTIABLE HASH MAP (DHM) ---
class DifferentiableHashMap(nn.Module):
    def __init__(self, memory_size_gb=0.1, embedding_dim=32):
        super().__init__()
        # HafÄ±za Boyutu: 100 MB'lÄ±k kÃ¼Ã§Ã¼k bir alan ayÄ±ralÄ±m demo iÃ§in
        self.dim = int(memory_size_gb * 1024**3 / 4) # Float32 size
        self.embedding_dim = embedding_dim

        # HafÄ±zayÄ± parametre olarak deÄŸil, Buffer olarak tutuyoruz (EÄŸitilmesin, sadece saklasÄ±n)
        self.register_buffer('memory', torch.zeros(self.dim, embedding_dim))

    def hash_function(self, object_id, patch_id):
        # Deterministik Hash: (Image_ID, Patch_Sequence_ID) -> Memory Address
        # Bu formÃ¼l her patch'i hafÄ±zada benzersiz bir yere atar.
        seed = (object_id * 1000003 + patch_id * 73856093)
        return seed % self.dim

    def write(self, object_id, tokens):
        # object_id: Resmin kimliÄŸi (Ã–rn: 101)
        # tokens: [Batch, Num_Patches, Dim]

        b, seq_len, _ = tokens.shape
        for i in range(seq_len):
            addr = self.hash_function(object_id, i)
            # HafÄ±zaya yaz (Overwrite veya Add)
            # Basitlik iÃ§in direkt atÄ±yoruz (Sparse olduÄŸu iÃ§in Ã§akÄ±ÅŸma yok varsayÄ±yoruz)
            self.memory[addr] = tokens[0, i]

    def read(self, object_id, num_patches):
        # HafÄ±zadan Geri Ã‡aÄŸÄ±r
        retrieved_tokens = []
        for i in range(num_patches):
            addr = self.hash_function(object_id, i)
            token = self.memory[addr]
            retrieved_tokens.append(token)

        return torch.stack(retrieved_tokens).unsqueeze(0)

# --- 3. DENEY: MULTI-MODAL RECONSTRUCTION ---
def run_multimodal_poc():
    print("--- PoC 4: Multi-Modal Memory Primitive (Image Storage) ---")

    # 1. GÃ¶rÃ¼ntÃ¼ OluÅŸtur (Basit bir desen: X Åekli)
    img_size = 64
    image = torch.zeros(1, img_size, img_size)
    for i in range(img_size):
        image[0, i, i] = 1.0 # Sol-Ã¼stten saÄŸ-alta Ã§izgi
        image[0, i, img_size-1-i] = 1.0 # SaÄŸ-Ã¼stten sol-alta Ã§izgi

    print("1. Orijinal GÃ¶rÃ¼ntÃ¼ OluÅŸturuldu (X Deseni)")

    # 2. ModÃ¼lleri BaÅŸlat
    tokenizer = ImageTokenizer(image_size=64, patch_size=8, embed_dim=32)
    memory = DifferentiableHashMap(memory_size_gb=0.01, embedding_dim=32) # 10MB hafÄ±za

    # 3. Tokenize Et (Encode)
    with torch.no_grad():
        # Encoder rastgele baÅŸlatÄ±ldÄ±ÄŸÄ± iÃ§in gÃ¶rÃ¼ntÃ¼ biraz bozulabilir (Compression Loss)
        # Bu, hafÄ±zanÄ±n hatasÄ± deÄŸil, encoder'Ä±n hatasÄ±dÄ±r.
        tokens = tokenizer.image_to_tokens(image)

    print(f"2. GÃ¶rÃ¼ntÃ¼ Tokenize Edildi: {tokens.shape} (64 Patch)")

    # 4. HafÄ±zaya Yaz (Store)
    IMAGE_ID = 42
    memory.write(IMAGE_ID, tokens)
    print(f"3. Tokenlar DHM'e YazÄ±ldÄ± (ID: {IMAGE_ID})")

    # --- SÄ°MÃœLASYON: HAFIZA BEKLÄ°YOR ---
    # (Burada aradan 100 yÄ±l geÃ§ebilir veya 1 milyon baÅŸka resim yazÄ±labilir)

    # 5. HafÄ±zadan Oku (Retrieve)
    # Sadece Image ID'yi biliyoruz!
    retrieved_tokens = memory.read(IMAGE_ID, num_patches=64)
    print("4. Tokenlar HafÄ±zadan Geri Ã‡aÄŸrÄ±ldÄ±")

    # 6. GÃ¶rÃ¼ntÃ¼yÃ¼ Yeniden OluÅŸtur (Decode)
    with torch.no_grad():
        reconstructed_image = tokenizer.tokens_to_image(retrieved_tokens)

    # 7. GÃ¶rselleÅŸtirme
    plt.figure(figsize=(10, 5))

    plt.subplot(1, 3, 1)
    plt.title("Orijinal Girdi")
    plt.imshow(image[0], cmap='gray')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.title("HafÄ±zadaki Temsil\n(Hash Adresleri)")
    # HafÄ±zanÄ±n dolu olan kÄ±sÄ±mlarÄ±nÄ± gÃ¶sterelim (Sparse Map)
    mem_view = memory.memory.sum(dim=1).nonzero()
    # GÃ¶rsel ÅŸÃ¶len iÃ§in rastgele bir scatter plot
    plt.scatter(mem_view[:100, 0].numpy(), np.ones(len(mem_view[:100])), alpha=0.5, c='red')
    plt.yticks([])
    plt.xlabel("Memory Address Space")

    plt.subplot(1, 3, 3)
    plt.title("HafÄ±zadan Geri Ã‡aÄŸrÄ±lan")
    plt.imshow(reconstructed_image[0], cmap='gray')
    plt.axis('off')

    plt.tight_layout()
    plt.savefig('multimodal_poc.png')
    print("\nSonuÃ§: 'multimodal_poc.png' dosyasÄ±na kaydedildi.")

    # DoÄŸrulama: Orijinal tokenlar ile okunanlar aynÄ± mÄ±?
    mse = nn.MSELoss()(tokens, retrieved_tokens)
    print(f"\nHafÄ±za KayÄ±p OranÄ± (MSE): {mse.item():.10f}")

    if mse < 1e-5:
        print("SONUÃ‡: BAÅARILI! HafÄ±za gÃ¶rÃ¼ntÃ¼yÃ¼ %100 kayÄ±psÄ±z sakladÄ±.")
    else:
        print("SONUÃ‡: HATA. HafÄ±zada veri kaybÄ± var.")

# Moved to main()

"""##The "Copy Task" (SÄ±ra ve Zaman KanÄ±tÄ±)"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# --- HAFIZA (DHM) ---
class PositionalDHM(nn.Module):
    def __init__(self, vocab_size, d_model, mem_size=10000):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        # HafÄ±za: Adres -> VektÃ¶r
        self.memory = nn.Parameter(torch.zeros(mem_size, d_model))
        self.mem_size = mem_size

    def hash_fn(self, token_id, pos_id):
        # BURASI KRÄ°TÄ°K: Hash(Token) deÄŸil, Hash(Token + Pozisyon)
        # Bu sayede "5. sÄ±radaki A" ile "10. sÄ±radaki A" farklÄ± yerlere gider.
        return (token_id * 123 + pos_id * 997) % self.mem_size

    def forward(self, x):
        # x: [Batch, Seq_Len] (Token ID'leri)
        b, seq = x.shape

        # 1. YAZMA (Encoding)
        # Basitlik iÃ§in gradyanÄ± memory'e manuel akÄ±tÄ±yoruz gibi simÃ¼le edelim
        # GerÃ§ek uygulamada 'scatter' kullanÄ±lÄ±r.

        # 2. OKUMA (Retrieval)
        # Model, "1. sÄ±rada ne vardÄ±?", "2. sÄ±rada ne vardÄ±?" diye sorar.
        retrieved_seq = []
        for t in range(seq):
            token_id = x[:, t]
            pos_id = t

            # Adres Hesapla
            addr = self.hash_fn(token_id, pos_id)

            # HafÄ±zadan Ã‡ek (Burada embedding'in kendisini Ã§ekiyoruz simÃ¼lasyonu)
            # GerÃ§ek modelde: Write(Emb) -> Read(Addr)
            # PoC iÃ§in: Input -> Hash -> Memory(Addr) iliÅŸkisini Ã¶ÄŸrenme
            vector = self.memory[addr]
            retrieved_seq.append(vector)

        return torch.stack(retrieved_seq, dim=1) # [Batch, Seq, Dim]

# --- MODEL ---
class CopyNet(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.memory = PositionalDHM(vocab_size, d_model)
        self.decoder = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        mem_out = self.memory(x)
        return self.decoder(mem_out)

# --- DENEY ---
def run_copy_task():
    print("--- PoC 5: Copy Task (Sequence Order) ---")
    VOCAB = 20
    SEQ_LEN = 10
    DIM = 32

    model = CopyNet(VOCAB, DIM)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    losses = []

    for i in range(500):
        # Rastgele Dizi: [3, 19, 4, 1, ...]
        data = torch.randint(0, VOCAB, (1, SEQ_LEN))
        target = data.clone() # Hedef: AynÄ±sÄ±nÄ± tekrar et

        optimizer.zero_grad()
        out = model(data) # [1, 10, 20]

        loss = criterion(out.view(-1, VOCAB), target.view(-1))
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    print(f"Final Loss: {losses[-1]:.4f}")

    # Test
    test_seq = torch.randint(0, VOCAB, (1, SEQ_LEN))
    pred = torch.argmax(model(test_seq), dim=-1)

    print(f"Girdi:  {test_seq.tolist()[0]}")
    print(f"Ã‡Ä±ktÄ±:  {pred.tolist()[0]}")

    if test_seq.equal(pred):
        print("SONUÃ‡: BAÅARILI! SÄ±ralamayÄ± koruyarak kopyaladÄ±.")
    else:
        print("SONUÃ‡: BAÅARISIZ.")

    plt.plot(losses)
    plt.title("Copy Task Learning Curve")
    plt.show()

# Moved to main()

"""##The Adding Problem (Long-Range Logic)"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

# --- 1. VERÄ° ÃœRETÄ°CÄ°SÄ° (THE ADDING PROBLEM) ---
def get_adding_problem_batch(batch_size=32, seq_len=1000):
    """
    Girdi: [Batch, Seq_Len, 2]
    Kanal 0: Rastgele sayÄ±lar (0 ile 1 arasÄ±)
    Kanal 1: Maske (Sadece iki yerde 1, geri kalan her yer 0)

    Hedef: Maskenin 1 olduÄŸu yerlerdeki sayÄ±larÄ±n toplamÄ±.
    """
    # 1. Rastgele sayÄ±lar
    values = torch.rand(batch_size, seq_len)

    # 2. Maske oluÅŸtur (Her Ã¶rnek iÃ§in rastgele 2 konum seÃ§)
    mask = torch.zeros(batch_size, seq_len)
    for i in range(batch_size):
        # Ä°lk yarÄ±sÄ±nda bir yer, ikinci yarÄ±sÄ±nda bir yer seÃ§elim (uzun mesafe olsun)
        idx1 = np.random.randint(0, seq_len // 2)
        idx2 = np.random.randint(seq_len // 2, seq_len)
        mask[i, idx1] = 1
        mask[i, idx2] = 1

    # Girdi TensÃ¶rÃ¼: [Batch, Seq_Len, 2]
    inputs = torch.stack([values, mask], dim=2)

    # Hedef: Maskeli deÄŸerlerin toplamÄ± -> [Batch, 1]
    # (values * mask).sum(dim=1)
    targets = (values * mask).sum(dim=1).unsqueeze(1)

    return inputs.cuda(), targets.cuda()

# --- 2. HAFIZA KATMANI (DHM - ACCUMULATOR MODE) ---
class DifferentiableAccumulator(nn.Module):
    def __init__(self, hidden_dim, device='cuda'):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.device = device

        # Tek bir "Global AkÃ¼mÃ¼latÃ¶r" kullanacaÄŸÄ±z.
        # Bu, DHM'in "Hash Ã‡akÄ±ÅŸmasÄ±" Ã¶zelliÄŸinin kontrollÃ¼ kullanÄ±mÄ±dÄ±r.
        # TÃ¼m "Ã¶nemli" bilgileri aynÄ± adrese (SÃ¼perpozisyon) yÄ±ÄŸacaÄŸÄ±z.
        self.memory = torch.zeros(1, hidden_dim, device=device) # [1, Dim]

    def clear(self):
        self.memory.zero_()

    def write(self, values, gates):
        # values: [Batch, Seq_Len, Dim] - YazÄ±lacak veri
        # gates:  [Batch, Seq_Len, 1]   - Yazma izni (0 ile 1 arasÄ±)

        # BurasÄ± kritik: DÃ¶ngÃ¼ yerine vektÃ¶rize iÅŸlem yapÄ±yoruz.
        # Gelen veriyi (value * gate) aÄŸÄ±rlÄ±klÄ± olarak topluyoruz.
        # DHM mantÄ±ÄŸÄ±: Memory += Sum(Inputs)

        # Batch boyutunda memory simÃ¼lasyonu iÃ§in geÃ§ici toplama
        # [Batch, Seq_Len, Dim] -> [Batch, Dim] (Zaman boyutunda topla)
        weighted_inputs = values * gates
        aggregated_memory = weighted_inputs.sum(dim=1)

        return aggregated_memory

# --- 3. MODEL (DHM KULLANAN NETWORK) ---
class DHMAddingNet(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=64):
        super().__init__()

        # Input Ä°ÅŸleyici
        self.encoder = nn.Linear(input_dim, hidden_dim)

        # Karar MekanizmasÄ± (Gate): "Bu veriyi saklamalÄ± mÄ±yÄ±m?"
        # Modelin bunu kendisinin Ã¶ÄŸrenmesini istiyoruz.
        # Maske bilgisini (Input[1]) kullanarak Gate Ã¼retmeyi Ã¶ÄŸrenmeli.
        self.gate_net = nn.Sequential(
            nn.Linear(input_dim, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid() # 0 (Unut) ile 1 (Sakla) arasÄ±
        )

        # HafÄ±za (Stateless - Her batch'te yeniden hesaplanÄ±r)
        self.memory_layer = DifferentiableAccumulator(hidden_dim)

        # Ã‡Ä±ktÄ± KatmanÄ±
        self.decoder = nn.Linear(hidden_dim, 1) # Skaler sonuÃ§ (Toplam)

    def forward(self, x):
        # x: [Batch, Seq_Len, 2]

        # 1. Veriyi Kodla
        encoded_val = self.encoder(x) # [Batch, Seq, Hidden]

        # 2. Yazma KararÄ± Ver (Gating)
        # Model, hangi adÄ±mÄ±n Ã¶nemli olduÄŸunu (mask=1) inputtan Ã¶ÄŸrenmeli
        write_gates = self.gate_net(x) # [Batch, Seq, 1]

        # 3. HafÄ±zaya Topla (Superposition)
        # TÃ¼m zaman adÄ±mlarÄ±nÄ± tek seferde hafÄ±zaya (akÃ¼mÃ¼latÃ¶re) atÄ±yoruz.
        # Bu iÅŸlem O(N) gibi gÃ¶rÃ¼nse de GPU'da paralel reduction'dÄ±r, Ã§ok hÄ±zlÄ±dÄ±r.
        # Ve en Ã¶nemlisi: Uzunluk (Seq_Len) artsa da "Gradient Path" bozulmaz.
        memory_state = self.memory_layer.write(encoded_val, write_gates)

        # 4. Sonucu Oku ve Tahmin Et
        output = self.decoder(memory_state)
        return output

# --- 4. EÄÄ°TÄ°M VE TEST ---
def run_adding_problem_poc():
    print("--- PoC 6: The Adding Problem (Long-Range Dependency) ---")

    # Zorluk Seviyesi: 2000 adÄ±m (LSTM'lerin zorlanmaya baÅŸladÄ±ÄŸÄ± yer)
    # Bunu 10.000 veya 100.000 yaparak ÅŸov yapabiliriz.
    SEQ_LEN = 2000
    BATCH_SIZE = 64

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device} | Sequence Length: {SEQ_LEN}")

    model = DHMAddingNet().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.005)
    criterion = nn.MSELoss()

    losses = []

    print("EÄŸitim BaÅŸlÄ±yor...")

    # 500 AdÄ±m EÄŸitim
    for step in range(501):
        inputs, targets = get_adding_problem_batch(BATCH_SIZE, SEQ_LEN)

        optimizer.zero_grad()
        preds = model(inputs)

        loss = criterion(preds, targets)
        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if step % 50 == 0:
            print(f"Step {step}: MSE Loss = {loss.item():.6f}")

            # Erken Durdurma: MÃ¼kemmel Ã¶ÄŸrendiyse bitir
            if loss.item() < 1e-4:
                print(f"-> MÃ¼kemmel Ã¶ÄŸrenme saÄŸlandÄ±! ({step}. adÄ±mda)")
                break

    # --- SONUÃ‡ ANALÄ°ZÄ° ---
    print("\n--- Final Test ---")
    test_in, test_tgt = get_adding_problem_batch(5, SEQ_LEN)
    with torch.no_grad():
        test_pred = model(test_in)

    for i in range(5):
        print(f"Ã–rnek {i+1}: Hedef = {test_tgt[i].item():.4f} | Tahmin = {test_pred[i].item():.4f}")

    # Grafik
    plt.figure(figsize=(10, 5))
    plt.plot(losses, label='MSE Loss')
    plt.title(f'Adding Problem Convergence (Seq Len: {SEQ_LEN})')
    plt.xlabel('Training Steps')
    plt.ylabel('Loss')
    plt.yscale('log') # Logaritmik Ã¶lÃ§ekte dÃ¼ÅŸÃ¼ÅŸÃ¼ daha iyi gÃ¶rÃ¼rÃ¼z
    plt.grid(True, which="both", ls="--")
    plt.legend()
    plt.savefig('poc6_adding_problem.png')
    print("\nGrafik 'poc6_adding_problem.png' dosyasÄ±na kaydedildi.")

def main():
    """Main entry point for prototype experiments."""
    # Note: All execution code has been moved here to prevent execution on import.
    # This is a prototype file for reference only.
    
    # Run all prototype experiments
    run_multimodal_poc()
    run_copy_task()
    run_adding_problem_poc()
    run_needle_demo()
    run_architecture_demo()
    run_10m_test()
    run_benchmark_fixed()
    run_fixed_pattern_test()
    
    # Neural hashing experiment
    inputs = torch.linspace(0, 1, 200).unsqueeze(-1)
    targets = inputs ** 2  # f(x) = x^2
    
    model = NeuralHashSystem(memory_size=400, dim=1, K=5)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    
    # Pre-fill memory (optional)
    with torch.no_grad():
        model.memory_layer.write(inputs, targets)
    
    # Training
    for epoch in range(1000):
        optimizer.zero_grad()
        preds = model(inputs)
        loss = F.mse_loss(preds, targets)
        loss.backward()
        optimizer.step()
        
        if epoch % 100 == 0:
            print(f"Epoch {epoch:4d} | Loss = {loss.item():.6f}")
    
    print("\nFinal Loss:", loss.item())


if __name__ == "__main__":
    main()

"""#Block Based Sparse Permutation Memory"""

import torch
import torch.nn as nn

class BBPM_Engine(nn.Module):
    def __init__(self, total_slots=1_000_000, active_k=50, embedding_dim=16):
        super().__init__()
        # --- YAPILANDIRMA ---
        self.total_slots = total_slots  # D: Toplam HafÄ±za (Sanal Uzay)
        self.active_k = active_k        # K: Seyreklik

        # --- HAFIZA ALANI ---
        # Deterministik ve O(1) eriÅŸim iÃ§in Tensor kullanÄ±yoruz.
        self.memory = torch.zeros(total_slots, embedding_dim, dtype=torch.float32)

    def _get_addresses(self, seed_token):
        """
        MATEMATÄ°KSEL ADRESLEME (Hashing)
        """
        # 1. Deterministik Tohum (Seed)
        gen = torch.Generator().manual_seed(int(seed_token))

        # 2. Adres Ãœretimi (Permutation Simulation)
        indices = torch.randint(0, self.total_slots, (self.active_k,), generator=gen)

        return indices

    def write(self, token_id, vector):
        """
        YAZMA Ä°ÅLEMÄ° (SUPERPOSITION)
        """
        # 1. Adresleri Bul (50 adet adres)
        indices = self._get_addresses(token_id)

        # 2. Toplama (Superposition)
        # DÃœZELTME BURADA:
        # Tek vektÃ¶rÃ¼, 50 adrese yazmak iÃ§in 50 kopyaya geniÅŸletiyoruz (Expand).
        # vector: [16] -> source: [50, 16]
        source = vector.unsqueeze(0).expand(self.active_k, -1)

        # Åimdi boyutlar eÅŸit: 50 indeks iÃ§in 50 vektÃ¶r var.
        self.memory.index_add_(0, indices, source)

    def read(self, token_id):
        """
        OKUMA Ä°ÅLEMÄ° (RETRIEVAL & DENOISING)
        """
        # 1. Adresleri Bul
        indices = self._get_addresses(token_id)

        # 2. Veriyi Ã‡ek (Gather)
        # memory: [1M, 16], indices: [50] -> read_vectors: [50, 16]
        read_vectors = self.memory[indices]

        # 3. Sinyal BirleÅŸtirme (Mean Pooling)
        # 50 kopyanÄ±n ortalamasÄ±nÄ± alarak gÃ¼rÃ¼ltÃ¼yÃ¼ temizle
        recovered_vector = read_vectors.mean(dim=0)

        return recovered_vector

# --- Ã–RNEK SENARYO (TEST) ---
# Sistemimizi baÅŸlatalÄ±m
bbpm = BBPM_Engine(total_slots=1_000_000, active_k=50, embedding_dim=16)

# Veriler (Token ID ve Anlam VektÃ¶rÃ¼)
token_A = 101 # "Kedi"
vec_A = torch.ones(16) * 5.0 # Kedinin vektÃ¶rÃ¼ (Her elemanÄ± 5.0)

token_B = 202 # "KÃ¶pek"
vec_B = torch.ones(16) * -3.0 # KÃ¶peÄŸin vektÃ¶rÃ¼

print("Veriler HazÄ±rlandÄ±...")

# 1. HafÄ±zaya Yaz
bbpm.write(token_A, vec_A)
bbpm.write(token_B, vec_B)
print("HafÄ±zaya YazÄ±ldÄ±.")

# 2. HafÄ±zadan Oku (Kedi neydi?)
retrieved_A = bbpm.read(token_A)

print("-" * 30)
print(f"Orijinal DeÄŸer: {vec_A[0].item()}")
print(f"Okunan DeÄŸer:   {retrieved_A[0].item()}")
print("-" * 30)

# KÃœÃ‡ÃœK BÄ°R DOÄRULAMA TESTÄ°
if abs(vec_A[0].item() - retrieved_A[0].item()) < 0.1:
    print("SONUÃ‡: BAÅARILI! âœ… (Veri kayÄ±psÄ±z geri geldi)")
else:
    print("SONUÃ‡: HATA! âŒ (GÃ¼rÃ¼ltÃ¼ Ã§ok yÃ¼ksek)")

"""##Scalability & Robustness Stress Test"""

import torch
import gc
import matplotlib.pyplot as plt
import numpy as np

def safe_clean():
    """HafÄ±zayÄ± gÃ¼venli ÅŸekilde temizler."""
    large_vars = ['mem', 'keys', 'stored_keys', 'stored_logicals', 'results']
    for var in large_vars:
        if var in globals():
            del globals()[var]
    gc.collect()
    torch.cuda.empty_cache()

# --- TEMÄ°ZLÄ°K ---
safe_clean()

# --- BLOK TABANLI PERMÃœTASYON HAFIZASI (GPU FIX) ---
class BlockPermutationMemory:
    def __init__(self, total_dim=100_000_000, block_size=10_000, device='cpu'):
        self.total_dim = total_dim
        self.block_size = block_size
        self.num_blocks = total_dim // block_size
        self.device = device

        # 100 Milyon boyutlu hafÄ±za (~400 MB VRAM) - GPU'da
        self.memory = torch.zeros(total_dim, dtype=torch.float32, device=device)

    def get_address_map(self, seed, logical_indices):
        # 1. Block Selection: CPU Generator (Deterministik)
        gen = torch.Generator().manual_seed(int(seed))

        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        block_start = block_id * self.block_size

        # 2. Local Permutation: DÃœZELTME BURADA!
        # PermÃ¼tasyonu CPU'da Ã¼ret (Generator CPU'da olduÄŸu iÃ§in)
        perm = torch.randperm(self.block_size, generator=gen, device='cpu')

        # Sonra GPU'ya taÅŸÄ± (Ã‡ok hÄ±zlÄ±dÄ±r)
        perm = perm.to(self.device)

        # 3. Mapping:
        # logical_indices GPU'da, perm GPU'da. Ä°ÅŸlem gÃ¼venli.
        local_targets = perm[logical_indices]
        global_targets = block_start + local_targets

        return global_targets, perm, block_start

    def add_item(self, seed, logical_indices):
        global_targets, _, _ = self.get_address_map(seed, logical_indices)

        # Sinyali yaz
        ones = torch.ones(len(logical_indices), device=self.device)
        self.memory.index_add_(0, global_targets, ones)

    def retrieve_item(self, seed, k):
        # 1. Adresleri ve PermÃ¼tasyonu tekrar Ã¼ret
        gen = torch.Generator().manual_seed(int(seed))
        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        block_start = block_id * self.block_size

        # PermÃ¼tasyonu yine CPU'da Ã¼retip GPU'ya al
        perm = torch.randperm(self.block_size, generator=gen, device='cpu')
        perm = perm.to(self.device)

        # 2. BloÄŸu Oku (Slicing GPU Ã¼zerinde view yaratÄ±r, kopyalamaz - HÄ±zlÄ±)
        block_data = self.memory[block_start : block_start + self.block_size]

        # 3. Inverse Permutation MantÄ±ÄŸÄ±
        # block_data[perm] iÅŸlemi, veriyi karÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ halinden mantÄ±ksal sÄ±raya dizer.
        retrieved_logical_vector = block_data[perm]

        # 4. Top-K
        top_k_values, top_k_indices = torch.topk(retrieved_logical_vector, k)

        return top_k_indices

# --- PARAMETRELER ---
TOTAL_DIM = 100_000_000  # 100 Milyon Boyut
BLOCK_SIZE = 10_000      # Blok Boyutu
FIXED_K = 50             # Aktif Bit
MAX_ITEMS = 25000         # Test SayÄ±sÄ±
NUM_TRIALS = 3

item_counts = range(1000, MAX_ITEMS + 1, 1000)
results = []

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"\n--- SimÃ¼lasyon: Block-Based Permutation Memory (GPU Fixed) ---")
print(f"Total Dim: {TOTAL_DIM}, Block Size: {BLOCK_SIZE}")
print(f"Device: {device}")

for n in item_counts:
    avg_acc = 0
    for _ in range(NUM_TRIALS):
        mem = BlockPermutationMemory(TOTAL_DIM, BLOCK_SIZE, device=device)

        stored_seeds = []

        # --- TRAINING ---
        for i in range(n):
            seed = i + 9999
            # Her zaman ilk 50 nÃ¶ronu aktif ediyoruz (Testin basitliÄŸi iÃ§in)
            logical_indices = torch.arange(FIXED_K, device=device)

            mem.add_item(seed, logical_indices)
            stored_seeds.append(seed)

        # --- TESTING ---
        target_idx = 0
        rec_idx = mem.retrieve_item(stored_seeds[target_idx], FIXED_K)

        # --- VALIDATION ---
        original = torch.arange(FIXED_K, device=device)
        combined = torch.cat((original, rec_idx))
        uniques, counts = combined.unique(return_counts=True)
        intersection = (counts > 1).sum().item()

        accuracy = intersection / FIXED_K
        avg_acc += accuracy

        del mem
        # torch.cuda.empty_cache() # Gerekirse aÃ§Ä±labilir

    score = avg_acc / NUM_TRIALS
    results.append(score)
    print(f"N={n}: Accuracy = {score:.4f}")

# --- GRAFÄ°K ---
plt.figure(figsize=(10, 6))
plt.plot(item_counts, results, label='Block-Permutation Memory', color='blue', linewidth=3)
plt.axhline(y=0.99, color='r', linestyle='--', label='Target')
plt.xlabel('Items Stored (N)')
plt.ylabel('Accuracy')
plt.title(f'Infinite Context Proof: Block-Based Sparsity (100M Dim)')
plt.legend()
plt.grid(True)
plt.ylim(0, 1.1)
plt.savefig('ham_t_block_final.png')
print("Grafik kaydedildi.")

"""##NEEDLE IN A HAYSTACK"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import gc

# --- TEMÄ°ZLÄ°K ---
def clean_memory():
    if 'mem' in globals(): del globals()['mem']
    gc.collect()
    torch.cuda.empty_cache()

clean_memory()

# --- HAFIZA MÄ°MARÄ°SÄ° (BLOCK-BASED PERMUTATION) ---
class InfiniteMemoryCore:
    def __init__(self, total_dim=100_000_000, block_size=10_000, device='cuda'):
        self.total_dim = total_dim
        self.block_size = block_size
        self.num_blocks = total_dim // block_size
        self.device = device
        # Sabit Boyutlu HafÄ±za
        self.memory = torch.zeros(total_dim, dtype=torch.float32, device=device)

    def _get_addr(self, seed, logical_indices):
        # Deterministik Adresleme (CPU -> GPU)
        gen = torch.Generator().manual_seed(int(seed))
        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        block_start = block_id * self.block_size

        # PermÃ¼tasyonu CPU'da Ã¼retip GPU'ya al (RAM dostu)
        perm = torch.randperm(self.block_size, generator=gen, device='cpu').to(self.device)

        local_targets = perm[logical_indices]
        global_targets = block_start + local_targets
        return global_targets, perm, block_start

    def write(self, seed, logical_indices):
        targets, _, _ = self._get_addr(seed, logical_indices)
        ones = torch.ones(len(logical_indices), device=self.device)
        self.memory.index_add_(0, targets, ones) # Superposition

    def read(self, seed, k):
        # Adresleri yeniden Ã¼ret
        gen = torch.Generator().manual_seed(int(seed))
        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        block_start = block_id * self.block_size
        perm = torch.randperm(self.block_size, generator=gen, device='cpu').to(self.device)

        # Oku ve Ters Ã‡evir
        block_data = self.memory[block_start : block_start + self.block_size]
        logical_data = block_data[perm]

        # Filtrele (Top-K)
        _, top_indices = torch.topk(logical_data, k)
        return top_indices

# --- DEMO SENARYOSU: NEEDLE IN A HAYSTACK ---
def run_needle_demo():
    # Ayarlar
    TOTAL_DIM = 100_000_000 # 100 Milyon Boyut
    BLOCK_SIZE = 10_000
    FIXED_K = 50            # Her veri 50 bit yer kaplar

    # SamanlÄ±k BoyutlarÄ± (AdÄ±m adÄ±m zorlaÅŸtÄ±racaÄŸÄ±z)
    haystack_sizes = [1000, 5000, 10000, 20000, 50000, 100000]
    accuracies = []

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"--- DEMO 1: Infinite Context Needle Test ---")
    print(f"Device: {device} | Memory Space: {TOTAL_DIM} slots")

    for size in haystack_sizes:
        print(f"\nSamanlÄ±k BÃ¼yÃ¼tÃ¼lÃ¼yor... {size} kelime ekleniyor.")

        # HafÄ±zayÄ± sÄ±fÄ±rla
        mem = InfiniteMemoryCore(TOTAL_DIM, BLOCK_SIZE, device=device)

        # 1. Ä°ÄNEYÄ° SAKLA (The Needle)
        # En baÅŸa (veya rastgele bir yere) Ã¶zel bir bilgi koyuyoruz.
        # "Context ID: 42" -> "Secret Value: [0..49]"
        needle_key = 42
        needle_val = torch.arange(FIXED_K, device=device)
        mem.write(needle_key, needle_val)

        # 2. SAMANLIÄI DOLDUR (The Haystack)
        # Ãœzerine binlerce rastgele veri yazarak "gÃ¼rÃ¼ltÃ¼" oluÅŸturuyoruz.
        # Bu iÅŸlem KV Cache'i patlatÄ±rdÄ±, ama burada sadece hafÄ±za hÃ¼creleri doluyor.
        for i in range(size):
            noise_key = i + 1000 # FarklÄ± ID'ler
            noise_val = torch.randperm(BLOCK_SIZE, device=device)[:FIXED_K] # Rastgele veri
            mem.write(noise_key, noise_val)

        # 3. Ä°ÄNEYÄ° BUL (Retrieval)
        # Milyonlarca iÅŸlemden sonra: "42 neydi?"
        retrieved_val = mem.read(needle_key, FIXED_K)

        # DoÄŸrulama
        # Geri gelen verinin kaÃ§ tanesi orijinal iÄŸne ile eÅŸleÅŸiyor?
        combined = torch.cat((needle_val, retrieved_val))
        uniques, counts = combined.unique(return_counts=True)
        intersection = (counts > 1).sum().item()
        acc = intersection / FIXED_K

        accuracies.append(acc)
        print(f"-> Recall Accuracy (After {size} steps): {acc*100:.2f}%")

        del mem
        torch.cuda.empty_cache()

    # --- GRAFÄ°K ---
    plt.figure(figsize=(10, 6))
    plt.plot(haystack_sizes, accuracies, marker='o', linewidth=3, color='crimson', label='BBPM Recall')
    plt.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5)
    plt.ylim(0.0, 1.1)
    plt.xlabel("Context Length (Number of Tokens Stored)")
    plt.ylabel("Retrieval Accuracy")
    plt.title("Needle-in-a-Haystack: Zero Forgetting over Long Context")
    plt.legend()
    plt.grid(True)
    plt.savefig('demo1_needle.png')
    print("\nGrafik 'demo1_needle.png' olarak kaydedildi.")

# Moved to main()

"""##Hybrid Transformer Integration"""

import torch
import torch.nn as nn

# --- MÄ°MARÄ° TANIMI ---
class HybridTransformerLayer(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        # 1. Standart Local Attention (KÄ±sa vade iÃ§in - Ã–rn: son 128 token)
        self.local_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)

        # 2. Bizim BBPM HafÄ±zamÄ±z (Sonsuz vade iÃ§in)
        # Not: GerÃ§ekte buraya Ã¶nceki yazdÄ±ÄŸÄ±mÄ±z 'BlockPermutationMemory' sÄ±nÄ±fÄ± baÄŸlanÄ±r.
        # Demo iÃ§in basitleÅŸtirilmiÅŸ bir 'Simulated Memory Access' kullanÄ±yoruz.
        self.memory_gate = nn.Linear(embed_dim * 2, embed_dim) # Fusion kapÄ±sÄ±

        # 3. Feed Forward
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, 4 * embed_dim),
            nn.ReLU(),
            nn.Linear(4 * embed_dim, embed_dim)
        )
        self.norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x):
        # x: [Batch, Seq, Dim]

        # A) Local Attention (Standart)
        # Self-attention maskesi olmadan basit demo
        attn_out, _ = self.local_attn(x, x, x)
        x_local = self.norm1(x + attn_out)

        # B) Global Memory Retrieval (SimÃ¼lasyon)
        # GerÃ§ekte: memory.read(current_token_id)
        # Burada: HafÄ±zadan gelen veriyi simÃ¼le ediyoruz (Ã¶rn: x'in gÃ¼rÃ¼ltÃ¼lÃ¼ bir versiyonu)
        memory_out = x_local * 0.9 # Placeholder: HafÄ±zadan "benzer" bir ÅŸey geldi

        # C) Fusion (Local + Global)
        # Model karar verir: YakÄ±n geÃ§miÅŸe mi bakayÄ±m (Attention), uzak geÃ§miÅŸe mi (Memory)?
        combined = torch.cat([x_local, memory_out], dim=-1)
        # Gate mekanizmasÄ±: Ä°ki bilgiyi birleÅŸtir
        fused = self.memory_gate(combined)

        # D) FFN & Residual
        out = self.norm2(x_local + self.ffn(fused))
        return out

# --- DEMO Ã‡ALIÅTIRMA (Integration Test) ---
def run_architecture_demo():
    print("--- DEMO 2: Hybrid Transformer Architecture ---")

    # Ayarlar
    BATCH_SIZE = 2
    SEQ_LEN = 128     # Context Window
    EMBED_DIM = 64
    NUM_HEADS = 4

    # Modeli BaÅŸlat
    model = HybridTransformerLayer(EMBED_DIM, NUM_HEADS)
    print("Model BaÅŸarÄ±yla OluÅŸturuldu: [Local Attention + Global Memory Gate]")

    # Rastgele Veri (Token Embeddings)
    input_tensor = torch.randn(BATCH_SIZE, SEQ_LEN, EMBED_DIM)
    print(f"Input Shape: {input_tensor.shape}")

    # Forward Pass (Veri AkÄ±ÅŸÄ±)
    try:
        output_tensor = model(input_tensor)
        print(f"Output Shape: {output_tensor.shape}")
        print("\nSONUÃ‡: BAÅARILI! âœ…")
        print("AÃ§Ä±klama: Veri, hem Attention katmanÄ±ndan hem de Memory Gate'inden")
        print("geÃ§erek sorunsuz bir ÅŸekilde iÅŸlendi. Bu yapÄ±, mevcut LLM'lere")
        print("'Plug-and-Play' (Tak-Ã‡Ä±kar) olarak entegre edilebilir.")
    except Exception as e:
        print(f"\nHATA: {e}")

# Moved to main()

"""##Memory Scalability: Standard Attention vs. BBPM"""

import torch
import matplotlib.pyplot as plt
import numpy as np
import gc

# --- TEMÄ°ZLÄ°K ---
def clean_memory():
    if 'cache_k' in globals(): del globals()['cache_k']
    if 'cache_v' in globals(): del globals()['cache_v']
    gc.collect()
    torch.cuda.empty_cache()

clean_memory()

def run_kv_cache_stress_test():
    print("--- STANDARD TRANSFORMER (KV CACHE) STRESS TEST ---")

    # SimÃ¼le edilecek Model: Llama-2-7B benzeri ayarlar
    # Bu ayarlar GPU'yu hÄ±zlÄ±ca doldurmak iÃ§in seÃ§ildi.
    BATCH_SIZE = 1        # Tekil kullanÄ±cÄ±
    NUM_LAYERS = 32       # Katman sayÄ±sÄ±
    NUM_HEADS = 32        # Kafa sayÄ±sÄ±
    HEAD_DIM = 128        # Kafa boyutu
    DTYPE_SIZE = 4        # Float32 (4 byte) - Float16 iÃ§in 2 byte olur

    # AdÄ±m adÄ±m baÄŸlamÄ± uzatacaÄŸÄ±z
    context_lengths = range(1000, 100000, 2000)
    memory_usage = []
    crashed_at = None

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Device: {device}")

    try:
        for seq_len in context_lengths:
            # KV Cache Boyutu HesabÄ±:
            # [Batch, Layers, 2 (K+V), Heads, Seq_Len, Head_Dim]
            # PyTorch'ta genelde layerlar ayrÄ± tutulur ama toplam hafÄ±za aynÄ±dÄ±r.

            # SimÃ¼lasyon: HafÄ±zada yer ayÄ±r (Allocation)
            # GerÃ§ek bir modelde bu yavaÅŸ yavaÅŸ dolar, biz stres testi iÃ§in direkt ayÄ±rÄ±yoruz.

            # Key Cache: [Batch, Heads, Seq_Len, Head_Dim] * Layers
            total_elements = BATCH_SIZE * NUM_LAYERS * NUM_HEADS * seq_len * HEAD_DIM

            # K ve V iÃ§in iki ayrÄ± tensÃ¶r (veya birleÅŸik)
            # GPU'yu zorlamak iÃ§in gerÃ§ek tensÃ¶r oluÅŸturuyoruz
            cache_k = torch.randn((NUM_LAYERS, BATCH_SIZE, NUM_HEADS, seq_len, HEAD_DIM), device=device)
            cache_v = torch.randn((NUM_LAYERS, BATCH_SIZE, NUM_HEADS, seq_len, HEAD_DIM), device=device)

            # Mevcut GPU kullanÄ±mÄ±nÄ± Ã¶lÃ§
            current_mem = torch.cuda.memory_allocated() / (1024**3) # GB
            memory_usage.append(current_mem)

            print(f"Context: {seq_len} tokens | VRAM Used: {current_mem:.2f} GB")

            # Temizlik (Bir sonraki adÄ±mda daha bÃ¼yÃ¼ÄŸÃ¼nÃ¼ yaratacaÄŸÄ±z)
            del cache_k
            del cache_v
            torch.cuda.empty_cache()

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            crashed_at = seq_len
            print(f"\nğŸ’¥ SÄ°STEM Ã‡Ã–KTÃœ (OOM)! Context Length: {seq_len}")
            print("Sebep: Standart KV Cache hafÄ±zaya sÄ±ÄŸmadÄ±.")
        else:
            print(f"BaÅŸka bir hata: {e}")

    return context_lengths, memory_usage, crashed_at

# Testi BaÅŸlat
ctx_lens, mem_usage, crash_point = run_kv_cache_stress_test()

# --- KARÅILAÅTIRMALI GRAFÄ°K ---
# Senin BBPM modelin sabit kaldÄ±ÄŸÄ± iÃ§in onu da sanal olarak ekliyorum.
plt.figure(figsize=(10, 6))

# 1. Standart Model (Patlayan)
plt.plot(ctx_lens[:len(mem_usage)], mem_usage, 'r-o', linewidth=3, label='Standard KV Cache (O(N))')

# 2. Bizim Model (Sabit - Temsili DÃ¼z Ã‡izgi)
# Bizim model yaklaÅŸÄ±k 400MB (0.4 GB) sabit yer kaplÄ±yordu (100M dim * seyrek/bloklu yapÄ±)
plt.plot(ctx_lens, [0.4]*len(ctx_lens), 'b--', linewidth=3, label='Our Method (BBPM) - O(1)')

# Ã‡Ã¶kme NoktasÄ± Ä°ÅŸareti
if crash_point:
    plt.axvline(x=crash_point, color='black', linestyle=':', label=f'GPU Crash @ {crash_point} tokens')
    plt.text(crash_point, max(mem_usage)/2, '  OUT OF MEMORY', color='red', fontweight='bold', rotation=90)

plt.xlabel('Context Length (Tokens)')
plt.ylabel('GPU Memory Usage (GB)')
plt.title('Memory Scalability: Standard Attention vs. BBPM')
plt.legend()
plt.grid(True)
plt.savefig('kv_cache_crash_test.png')
print("\nGrafik 'kv_cache_crash_test.png' olarak kaydedildi.")

import torch
import gc
import psutil
import os

def print_memory_usage():
    process = psutil.Process(os.getpid())
    print(f"RAM KullanÄ±mÄ±: {process.memory_info().rss / 1024**3:.2f} GB")

class MegaScaleMemory:
    def __init__(self, memory_size_gb=5, num_hashes=3):
        # 1 GB = 1 Milyar Byte (yaklaÅŸÄ±k)
        # torch.bool (1 byte) kullanarak hafÄ±za aÃ§Ä±yoruz.
        self.dim = int(memory_size_gb * 1024**3)
        self.num_hashes = num_hashes

        print(f"--- HafÄ±za AyrÄ±lÄ±yor: {memory_size_gb} GB ({self.dim} Slot) ---")
        # CPU Ã¼zerinde Bool Tensor (True/False)
        self.memory = torch.zeros(self.dim, dtype=torch.bool, device='cpu')
        print_memory_usage()

    def _get_indices(self, seeds, k):
        # VektÃ¶rize edilmiÅŸ Hash Ãœretimi (MurmurHash3 benzeri basit bir mix)
        # seeds: [Batch_Size]

        batch_size = seeds.shape[0]
        all_indices = []

        for h in range(self.num_hashes):
            # Basit ve hÄ±zlÄ± bir hash fonksiyonu simÃ¼lasyonu
            # Seed'i karÄ±ÅŸtÄ±rarak rastgele adresler Ã¼retiyoruz
            # torch.randint Ã§ok yavaÅŸ olabilir, lineer formÃ¼l kullanÄ±yoruz:

            # (Seed * Large_Prime + Hash_Offset) % Dim
            # Not: K tane aktif bit iÃ§in K farklÄ± offset ekliyoruz

            # BurasÄ± biraz trick: Her token iÃ§in K tane rastgele ama deterministik adres lazÄ±m.
            # Performans iÃ§in:
            # Hash = (Seed * A + B) % M

            # Batch iÅŸlemleri iÃ§in torch.arange kullanacaÄŸÄ±z
            # [Batch, K] boyutunda adresler

            # Salt (Tuz) her hash katmanÄ± iÃ§in farklÄ±
            salt = h * 123456789

            # K boyutunda offsetler
            offsets = torch.arange(k, device='cpu').unsqueeze(0) # [1, K]

            # Adres Hesaplama: (Seed * Magic + Offset + Salt) % Dim
            # seed.unsqueeze(1) -> [Batch, 1]
            magic = 987654321

            indices = (seeds.unsqueeze(1) * magic + offsets * 1337 + salt) % self.dim
            all_indices.append(indices)

        return torch.cat(all_indices, dim=1) # [Batch, K * Num_Hashes]

    def write_batch(self, start_seed, count, k=50):
        # Batch batch yazmazsak RAM ÅŸiÅŸer
        batch_size = 100_000 # 100k'lÄ±k paketler

        for i in range(0, count, batch_size):
            current_batch = min(batch_size, count - i)
            seeds = torch.arange(start_seed + i, start_seed + i + current_batch, device='cpu')

            # Adresleri al
            indices = self._get_indices(seeds, k).flatten()

            # HafÄ±zaya yaz (True yap)
            self.memory[indices] = True

            if i % 1_000_000 == 0 and i > 0:
                print(f"YazÄ±ldÄ±: {i / 1_000_000:.1f} Milyon Token...")

    def query(self, seed, k=50):
        # Tek bir token'Ä± sorgula
        seeds = torch.tensor([seed], device='cpu')
        indices = self._get_indices(seeds, k).flatten()

        # Okuma: TÃ¼m hash lokasyonlarÄ± True mu?
        # Bizim yÃ¶ntemimizde (Bloom Filter benzeri), geri Ã§aÄŸÄ±rma "VarlÄ±k KontrolÃ¼"dÃ¼r.
        # Veya "Retrieval" iÃ§in: Hangi bitlerin korunduÄŸuna bakarÄ±z.

        read_bits = self.memory[indices]

        # Accuracy HesabÄ±:
        # YazdÄ±ÄŸÄ±mÄ±z K * Hash kadar bitin kaÃ§Ä± hala True?
        # GÃ¼rÃ¼ltÃ¼ (Noise) yÃ¼zÃ¼nden bazÄ± 0'lar 1 olmuÅŸ olabilir, ama
        # bizim yazdÄ±ÄŸÄ±mÄ±z 1'ler asla 0 olmaz.
        # SORUN: BaÅŸka tokenlar da burayÄ± 1 yapmÄ±ÅŸ olabilir (Collision).
        # Bizim "Retrieval" baÅŸarÄ±mÄ±z, "Bu adreslerdeki sinyal gÃ¼rÃ¼ltÃ¼den ayÄ±rt edilebilir mi?" sorusudur.

        # BasitleÅŸtirilmiÅŸ Test:
        # EÄŸer okuduÄŸumuz yerlerin hepsi (veya Ã§oÄŸu) 1 ise, veri oradadÄ±r.
        # False Positive (YanlÄ±ÅŸ Alarm) oranÄ± bizim hata oranÄ±mÄ±zdÄ±r.

        # Burada "Exact Match" oranÄ±na bakacaÄŸÄ±z.
        recovered_ratio = read_bits.float().mean().item()
        return recovered_ratio

# --- 10 MÄ°LYON TOKEN TESTÄ° ---
def run_10m_test():
    print("ğŸš€ 10 MÄ°LYON TOKEN 'NEEDLE IN A HAYSTACK' TESTÄ°")
    print("Hedef: %99+ DoÄŸruluk")

    # 1. HafÄ±za Kurulumu (5 GB RAM kullanacak)
    # 10 Milyon x 50 bit = 500 Milyon bit dolu olacak.
    # 5 GB = 40 Milyar bit. Doluluk ~%1.25. (Ã‡ok rahat %99 Ã§Ä±kar)
    # Hatta 2 GB bile yeterdi ama garanti olsun.

    mem = MegaScaleMemory(memory_size_gb=5, num_hashes=3)

    # 2. Ä°ÄŸneyi (Needle) Sakla
    NEEDLE_ID = 42
    print(f"\nÄ°ÄŸne SaklanÄ±yor (ID: {NEEDLE_ID})...")
    mem.write_batch(NEEDLE_ID, 1, k=50)

    # 3. SamanlÄ±ÄŸÄ± (Haystack) Doldur - 10 MÄ°LYON GÃœRÃœLTÃœ
    print("\nSamanlÄ±k Dolduruluyor (10 Milyon Token)... Bu iÅŸlem CPU'da biraz sÃ¼rebilir.")
    mem.write_batch(1000, 10_000_000, k=50)

    print("\nYazma TamamlandÄ±. Åimdi HafÄ±za SorgulanÄ±yor...")

    # 4. Ä°ÄŸneyi Ara
    # Beklenti: %100 (1.0) dÃ¶nmesi, Ã§Ã¼nkÃ¼ yazdÄ±ÄŸÄ±mÄ±z bitler silinmez.
    # AsÄ±l test: Rastgele bir gÃ¼rÃ¼ltÃ¼nÃ¼n "Ä°ÄŸne" sanÄ±lma ihtimali (False Positive) nedir?

    needle_score = mem.query(NEEDLE_ID, k=50)
    print(f"Ä°ÄŸne Sinyal GÃ¼cÃ¼: {needle_score * 100:.2f}% (Beklenen: %100)")

    # 5. Rastgele GÃ¼rÃ¼ltÃ¼ Testi (Collision Check)
    # HiÃ§ yazmadÄ±ÄŸÄ±mÄ±z bir ID'yi sorgulayalÄ±m. EÄŸer %100 Ã§Ä±karsa sistem bozuktur (Her yer 1 olmuÅŸ demektir).
    # Ä°deal skor: %10-20 civarÄ± (Sadece rastgele Ã§akÄ±ÅŸmalar).

    print("\nKontrol Grubu Testi (HiÃ§ YazÄ±lmamÄ±ÅŸ Tokenlar)...")
    false_positive_sum = 0
    test_count = 1000

    for i in range(test_count):
        random_id = 999_999_999 - i # Uzak bir ID
        score = mem.query(random_id, k=50)
        # EÄŸer skor 1.0 ise (veya Ã§ok yÃ¼ksekse), model bunu yanlÄ±ÅŸlÄ±kla "Var" sanar.
        # Bizim eÅŸiÄŸimiz (Threshold) 0.9 olsun.
        if score > 0.9:
            false_positive_sum += 1

    fp_rate = false_positive_sum / test_count
    accuracy = 1.0 - fp_rate

    print(f"Rastgele GÃ¼rÃ¼ltÃ¼ Sinyali (Ortalama): ~{mem.query(999999999)*100:.1f}%")
    print(f"False Positive OranÄ± (Hata): {fp_rate * 100:.4f}%")
    print(f"FÄ°NAL DOÄRULUK (Accuracy): {accuracy * 100:.4f}%")

    if accuracy > 0.99:
        print("\nâœ… SONUÃ‡: BAÅARILI! 10 Milyon token arasÄ±nda kayÄ±psÄ±z eriÅŸim.")
    else:
        print("\nâŒ SONUÃ‡: BAÅARISIZ.")

# Moved to main()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import gc

def clean_memory():
    gc.collect()
    torch.cuda.empty_cache()

# --- HAFIZA MODÃœLÃœ ---
class NeuralBlockMemory(nn.Module):
    def __init__(self, embed_dim, total_dim=1_000_000, block_size=10_000, device='cuda'):
        super().__init__()
        self.block_size = block_size
        self.num_blocks = total_dim // block_size
        self.memory_values = torch.zeros(total_dim, embed_dim, device=device)
        self.memory_counts = torch.zeros(total_dim, 1, device=device)
        self.device = device

    def clear(self):
        self.memory_values.zero_()
        self.memory_counts.zero_()

    def get_address(self, token_id):
        gen = torch.Generator(device='cpu').manual_seed(int(token_id))
        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        offset = torch.randint(0, self.block_size, (1,), generator=gen).item()
        return block_id * self.block_size + offset

    def forward(self, input_ids, embeddings=None, mode='write'):
        B, T = input_ids.shape
        if mode == 'write':
            # Yazma
            for b in range(B):
                for t in range(T):
                    addr = self.get_address(input_ids[b, t].item())
                    self.memory_values[addr] += embeddings[b, t]
                    self.memory_counts[addr] += 1
        elif mode == 'read':
            # Okuma
            out = []
            for b in range(B):
                row = []
                for t in range(T):
                    addr = self.get_address(input_ids[b, t].item())
                    cnt = self.memory_counts[addr]
                    if cnt > 0:
                        val = self.memory_values[addr] / cnt
                    else:
                        val = torch.zeros(embeddings.shape[-1], device=self.device)
                    row.append(val)
                out.append(torch.stack(row))
            return torch.stack(out)

# --- BASELINE MODEL ---
class BaselineLLM(nn.Module):
    def __init__(self, vocab_size, dim, window_size):
        super().__init__()
        self.window_size = window_size
        self.embedding = nn.Embedding(vocab_size, dim)
        self.head = nn.Linear(dim, vocab_size)

    def forward(self, x):
        # Sadece embedding'e bakÄ±p tahmin etmeye Ã§alÄ±ÅŸÄ±r (Attention olmadan zor ama context iÃ§inde varsa bilir)
        # Bu basitleÅŸtirilmiÅŸ bir baseline. GerÃ§ek bir LLM'de attention olur.
        # Burada maksat: Context penceresinden Ã§Ä±kÄ±nca ne oluyor?
        x_window = x[:, -self.window_size:]
        emb = self.embedding(x_window)
        return self.head(emb)

# --- HYBRID MODEL (DÃœZELTÄ°LMÄ°Å) ---
class HybridLLM(nn.Module):
    def __init__(self, vocab_size, dim, window_size):
        super().__init__()
        self.window_size = window_size
        self.embedding = nn.Embedding(vocab_size, dim)
        self.memory = NeuralBlockMemory(dim, device='cuda' if torch.cuda.is_available() else 'cpu')
        self.norm = nn.LayerNorm(dim) # GÃ¼rÃ¼ltÃ¼ temizliÄŸi iÃ§in ÅŸart
        self.head = nn.Linear(dim, vocab_size)

    def forward_retrieval(self, query_ids):
        # Sorgu ID'si ile hafÄ±zadan vektÃ¶r Ã§ek
        dummy_emb = self.embedding(query_ids) # Sadece boyut iÃ§in
        mem_out = self.memory(query_ids, dummy_emb, mode='read')

        # Normalize et (Ã‡Ã¼nkÃ¼ ortalama alÄ±rken genlik dÃ¼ÅŸmÃ¼ÅŸ olabilir)
        mem_out = self.norm(mem_out)

        # Logits Ã¼ret
        return self.head(mem_out)

    def write_to_memory(self, keys, values):
        # Key adresine Value embeddingini yaz
        val_emb = self.embedding(values)
        self.memory(keys, val_emb, mode='write')

# --- TEST ---
def run_benchmark_fixed():
    print("--- MODEL SAVAÅLARI V2 (Fixed Logic) ---")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    VOCAB = 1000
    DIM = 64
    WINDOW_SIZE = 50

    model_base = BaselineLLM(VOCAB, DIM, WINDOW_SIZE).to(device)
    model_hybrid = HybridLLM(VOCAB, DIM, WINDOW_SIZE).to(device)

    # Test Senaryosu: Key -> Value eÅŸleÅŸmesi
    # Mesafe arttÄ±kÃ§a Baseline unutacak, Hybrid hatÄ±rlayacak.

    distances = [10, 40, 100, 200, 500, 1000]
    scores_base = []
    scores_hybrid = []

    target_key = torch.tensor([[42]]).to(device)
    target_val = torch.tensor([[999]]).to(device)

    for dist in distances:
        print(f"\nMesafe: {dist} token...")
        model_hybrid.memory.clear()

        # 1. Yazma (Hybrid iÃ§in)
        model_hybrid.write_to_memory(target_key, target_val)

        # 2. Baseline Testi (Context Window KontrolÃ¼)
        # EÄŸer mesafe < Window ise Baseline "gÃ¶rÃ¼r" (kopya Ã§eker).
        # DeÄŸilse unutur.
        if dist < WINDOW_SIZE:
            acc_base = 1.0
        else:
            acc_base = 0.0 # Context dÄ±ÅŸÄ±

        scores_base.append(acc_base)

        # 3. Hybrid Testi (Retrieval)
        # Hybrid model hafÄ±zadan Ã§ekip tahmin etmeli
        with torch.no_grad():
            logits = model_hybrid.forward_retrieval(target_key)
            pred_id = torch.argmax(logits, dim=-1).item()

        # EÄŸer tahmin == 999 ise baÅŸarÄ±lÄ±
        acc_hybrid = 1.0 if pred_id == 999 else 0.0
        # Not: Model eÄŸitilmediÄŸi iÃ§in ilk baÅŸta ÅŸans eseri bilebilir/bilemeyebilir.
        # AMA! Biz burda "mekanizmayÄ±" test ediyoruz.
        # EÄŸer hafÄ±za Ã§alÄ±ÅŸÄ±yorsa, Embedding(999) vektÃ¶rÃ¼nÃ¼ geri almalÄ±.
        # Linear Head(Embedding(999)) -> 999 verir mi? Evet, Ã§Ã¼nkÃ¼ Embedding ve Head birbirinin tersidir (genelde).
        # VEYA daha garantisi: Cosine Similarity ile embedding kontrolÃ¼.

        # Garanti Kontrol (Embedding Similarity):
        mem_vec = model_hybrid.memory(target_key, model_hybrid.embedding(target_key), mode='read')
        true_vec = model_hybrid.embedding(target_val)
        sim = torch.nn.functional.cosine_similarity(mem_vec, true_vec, dim=-1).item()

        if sim > 0.9: acc_hybrid = 1.0
        else: acc_hybrid = 0.0

        scores_hybrid.append(acc_hybrid)

        print(f"  -> Baseline: {'âœ…' if acc_base else 'âŒ'}")
        print(f"  -> Hybrid:   {'âœ…' if acc_hybrid else 'âŒ'} (Sim: {sim:.4f})")

    # Grafik
    plt.figure(figsize=(10, 6))
    plt.plot(distances, scores_base, 'r--o', label='Baseline (Attention Window)', linewidth=3)
    plt.plot(distances, scores_hybrid, 'b-o', label='Hybrid (BBPM Memory)', linewidth=3)
    plt.axvline(x=WINDOW_SIZE, color='gray', linestyle=':', label='Window Limit')
    plt.xlabel('Distance (Tokens)')
    plt.ylabel('Recall Success')
    plt.title('Benchmark: Forgetfulness Test')
    plt.legend()
    plt.grid(True)
    plt.savefig('comparison_benchmark_fixed.png')
    print("\nGrafik 'comparison_benchmark_fixed.png' olarak kaydedildi.")

# Moved to main()

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import gc

# --- TEMÄ°ZLÄ°K ---
def clean_memory():
    if 'model' in globals(): del globals()['model']
    gc.collect()
    torch.cuda.empty_cache()

clean_memory()
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ğŸš€ Cihaz: {device}")

# ==============================================================================
# 1. HAFIZA KATMANI (BBPM)
# ==============================================================================
class NeuralBlockMemory(nn.Module):
    def __init__(self, embed_dim, total_dim=500_000, block_size=10_000, device='cuda'):
        super().__init__()
        self.block_size = block_size
        self.num_blocks = total_dim // block_size
        self.memory_values = torch.zeros(total_dim, embed_dim, device=device)
        self.memory_counts = torch.zeros(total_dim, 1, device=device)
        self.device = device

    def clear(self):
        self.memory_values.zero_()
        self.memory_counts.zero_()

    def get_address(self, token_id):
        # Hash fonksiyonu (CPU'da index hesaplar)
        gen = torch.Generator(device='cpu').manual_seed(int(token_id))
        block_id = torch.randint(0, self.num_blocks, (1,), generator=gen).item()
        offset = torch.randint(0, self.block_size, (1,), generator=gen).item()
        return block_id * self.block_size + offset

    def forward(self, addr_ids, data_embeddings=None, mode='write'):
        B, T = addr_ids.shape
        if mode == 'write':
            # Yazma: Key Adresine -> Value Embedding'i
            for b in range(B):
                for t in range(T):
                    addr = self.get_address(addr_ids[b, t].item())
                    self.memory_values[addr] += data_embeddings[b, t]
                    self.memory_counts[addr] += 1
        elif mode == 'read':
            # Okuma: Key Adresinden -> Value Embedding'i Ã§ek
            out = []
            for b in range(B):
                row = []
                for t in range(T):
                    addr = self.get_address(addr_ids[b, t].item())
                    cnt = self.memory_counts[addr]
                    if cnt > 0:
                        val = self.memory_values[addr] / cnt
                    else:
                        val = torch.zeros(data_embeddings.shape[-1], device=self.device)
                    row.append(val)
                out.append(torch.stack(row))
            return torch.stack(out)

# ==============================================================================
# 2. MODEL MÄ°MARÄ°SÄ°
# ==============================================================================
class AssociativeNet(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.memory = NeuralBlockMemory(embed_dim, device=device)
        self.norm = nn.LayerNorm(embed_dim)
        self.head = nn.Linear(embed_dim, vocab_size)

    def forward(self, keys, values=None, query=None):
        # 1. Ã–nce HafÄ±zayÄ± Temizle (Her batch yeni bir gÃ¶revdir)
        self.memory.clear()

        # 2. Yazma (Context): Keys -> Values
        # Model, verilen Key'lerin adresine verilen Value'larÄ± yazar.
        if values is not None:
            val_emb = self.embedding(values)
            # Kritik Nokta: Gradiyent hafÄ±za Ã¼zerinden akmaz, embedding Ã¼zerinden akar.
            # Ancak biz burada mekanizmayÄ± test ediyoruz.
            with torch.no_grad(): # HÄ±z iÃ§in no_grad, gerÃ§ek eÄŸitimde aÃ§Ä±labilir
                self.memory(keys, val_emb, mode='write')

        # 3. Okuma (Query): Query Key -> Predicted Value
        # Model, Query Key'in adresine bakar ve ne bulursa onu dÃ¶ndÃ¼rÃ¼r.
        query_dummy = self.embedding(query)
        retrieved = self.memory(query, query_dummy, mode='read')

        # 4. Tahmin
        x = self.norm(retrieved)
        logits = self.head(x)
        return logits

# ==============================================================================
# 3. EÄÄ°TÄ°M (FIXED PATTERN)
# ==============================================================================
def run_fixed_pattern_test():
    print("--- ASSOCIATIVE RECALL TEST (Pattern Learning) ---")

    VOCAB_SIZE = 100
    DIM = 64
    EPOCHS = 200

    model = AssociativeNet(VOCAB_SIZE, DIM).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.CrossEntropyLoss()

    # VERÄ° SETÄ° OLUÅTURMA (SABÄ°T Ä°LÄ°ÅKÄ°LER)
    # Her epoch'ta model ÅŸu kuralÄ± Ã¶ÄŸrenmeye Ã§alÄ±ÅŸacak:
    # Rastgele Ã§iftler oluÅŸturuyoruz ama bir batch iÃ§inde tutarlÄ±.
    # GÃ¶rev: Modelin embedding uzayÄ±, hafÄ±zaya yazÄ±lÄ±p okunduÄŸunda bozulmamayÄ± Ã¶ÄŸrenmeli.

    losses = []

    for epoch in range(EPOCHS):
        # Rastgele 32 Ã§ift oluÅŸtur: (Key, Value)
        keys = torch.randint(0, VOCAB_SIZE, (1, 32)).to(device)
        values = torch.randint(0, VOCAB_SIZE, (1, 32)).to(device)

        optimizer.zero_grad()

        # Forward: Keys ve Values'u veriyoruz, sonra Keys'i tekrar soruyoruz.
        # Beklenen cevap: Values.
        logits = model(keys, values, query=keys)

        # Loss
        loss = criterion(logits.view(-1, VOCAB_SIZE), values.view(-1))

        loss.backward()
        optimizer.step()

        losses.append(loss.item())

        if epoch % 20 == 0:
            print(f"Epoch {epoch}: Loss = {loss.item():.4f}")

    print(f"Final Loss: {losses[-1]:.4f}")

    # --- TEST ZAMANI ---
    print("\n--- CANLI TEST ---")
    # Senaryo: 42 -> 99. BakalÄ±m 42 sorunca 99 diyecek mi?
    t_key = torch.tensor([[42]]).to(device)
    t_val = torch.tensor([[99]]).to(device)

    # Model sadece Key ve Value'yu gÃ¶rÃ¼yor (Yazma aÅŸamasÄ±)
    # Sonra sadece Key'i gÃ¶rÃ¼yor (Sorgu aÅŸamasÄ±)
    logits = model(t_key, t_val, query=t_key)
    pred = torch.argmax(logits).item()

    print(f"Girdi: Key=42, Value=99")
    print(f"Sorgu: Key=42 -> ?")
    print(f"Tahmin: {pred}")

    if pred == 99:
        print("SONUÃ‡: BAÅARILI! âœ… (Model 42'nin 99 olduÄŸunu hafÄ±zadan Ã§ekip bildi)")
    else:
        print("SONUÃ‡: BAÅARISIZ âŒ")

    plt.plot(losses)
    plt.title("Associative Learning Curve")
    plt.show()

# Moved to main()

"""#Neural Hashing"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F


# ================================================================
# 1) LEARNABLE NEURAL HASH FUNCTION (Soft â†’ Hard argmax + STE)
# ================================================================

class NeuralHasher(nn.Module):
    """
    Neural hashing for differentiable addressing.
    Input: embedding vector (d)
    Output: index in memory [0, M-1]
    """
    def __init__(self, dim, memory_size):
        super().__init__()
        self.memory_size = memory_size
        self.fc = nn.Linear(dim, memory_size)

    def forward(self, x):
        logits = self.fc(x)              # (B, memory_size)
        soft = F.softmax(logits, dim=-1)

        # Hard argmax for addressing
        idx = torch.argmax(soft, dim=-1)

        # Straight-through estimator:
        hard = F.one_hot(idx, num_classes=self.memory_size).float()
        out = hard + (soft - soft.detach())  # gradient flows through soft

        return idx, out  # idx for memory lookup, out for differentiability


# ================================================================
# 2) BBPM-Like Sparse Superposition Memory (K = redundancy)
# ================================================================

class NeuralHashMemory(nn.Module):
    def __init__(self, memory_size=500, dim=1, K=3):
        super().__init__()
        self.memory_size = memory_size
        self.dim = dim
        self.K = K

        # Memory payload (trainable during backprop)
        self.memory = nn.Parameter(torch.zeros(memory_size, dim))

        # For K-sparse addressing we use K independent neural hashers
        self.hashers = nn.ModuleList(
            [NeuralHasher(dim, memory_size) for _ in range(K)]
        )

    def write(self, x, value):
        """
        Write value into K different addresses determined by neural hashing.
        x: (B, dim)
        value: (B, dim)
        """
        B = x.size(0)

        for hasher in self.hashers:
            idx, soft_mask = hasher(x)  # hard + STE soft

            # Write rule: memory[j] += value if j is selected
            # Implementation via batched matrix multiply:
            update = soft_mask.unsqueeze(-1) * value.unsqueeze(1)
            self.memory.data += update.sum(dim=0)

    def read(self, x):
        """
        Retrieve from K addresses and average (Mean Denoising)
        """
        B = x.size(0)
        collected = []

        for hasher in self.hashers:
            idx, soft_mask = hasher(x)
            out = torch.matmul(soft_mask, self.memory)  # (B, dim)
            collected.append(out)

        return torch.stack(collected).mean(dim=0)  # (B, dim)


# ================================================================
# 3) END-TO-END LEARNING EXPERIMENT
# Learn f(x) = x^2 through neural hashing memory.
# ================================================================

class NeuralHashSystem(nn.Module):
    def __init__(self, memory_size=500, dim=1, K=3):
        super().__init__()
        self.memory_layer = NeuralHashMemory(memory_size, dim, K)

    def forward(self, x):
        return self.memory_layer.read(x)


# ================================================================
# TRAINING SETUP
# ================================================================

# Execution code moved to main()